#!/usr/bin/env python3
"""
ABOUTME: Interactive graph reasoning demo - shows all features working.
ABOUTME: Uses Python directly (no API) for maximum reliability.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import get_db_context
from api.services.graph_reasoning_service import GraphReasoningService
import json

def print_section(title):
    print("\n" + "=" * 80)
    print(f"ðŸ“Š {title}")
    print("=" * 80)

def main():
    print("=" * 80)
    print("ðŸŽ¯ GRAPH REASONING - COMPREHENSIVE DEMO")
    print("=" * 80)
    print("\nThis demo shows all graph reasoning features working via Python")
    print()
    
    # Initialize
    print_section("1. Initialize Service")
    with get_db_context() as conn:
        service = GraphReasoningService(db_connection=conn)
        print("âœ… Service initialized")
        
        # Build graph
        print_section("2. Build Graph (1000 nodes)")
        print("Building graph from database...")
        graph = service.build_graph_from_database(limit=1000)
        print(f"âœ… Graph built: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges")
        
        # Fast statistics
        print_section("3. Fast Statistics (no betweenness)")
        print("Computing statistics...")
        stats = service.compute_graph_statistics(compute_betweenness=False)
        print(f"âœ… Statistics computed:")
        print(f"   â€¢ Nodes: {stats['num_nodes']}")
        print(f"   â€¢ Edges: {stats['num_edges']}")
        print(f"   â€¢ Density: {stats['density']:.6f}")
        print(f"   â€¢ Average Clustering: {stats['avg_clustering']:.4f}")
        print(f"   â€¢ Connected: {stats['is_connected']}")
        print(f"   â€¢ Components: {stats['num_components']}")
        print(f"   â€¢ Average Degree: {stats['avg_degree']:.2f}")
        print(f"   â€¢ Max Degree: {stats['max_degree']}")
        
        # Compute embeddings
        print_section("4. Compute Node Embeddings")
        print("Computing 128-dimensional embeddings for all nodes...")
        embeddings = service.compute_node_embeddings(dimension=128)
        print(f"âœ… Embeddings computed for {len(embeddings)} nodes")
        
        # Find similar people
        print_section("5. Find Similar People")
        if len(embeddings) >= 2:
            # Get a node with connections
            test_node = None
            for node_id in list(graph.nodes())[:50]:
                if graph.degree(node_id) > 0:
                    test_node = node_id
                    break
            
            if test_node:
                test_name = graph.nodes[test_node].get('full_name', 'Unknown')
                print(f"Finding people similar to: {test_name}")
                
                similar = service.find_similar_nodes(test_node, top_k=5, min_similarity=0.3)
                
                if similar:
                    print(f"\nâœ… Found {len(similar)} similar people:")
                    for i, person in enumerate(similar, 1):
                        print(f"   {i}. {person['full_name']}")
                        print(f"      Similarity: {person['similarity_score']:.4f}")
                        if person['github_username']:
                            print(f"      GitHub: @{person['github_username']}")
                else:
                    print("   No similar people found (may need to lower threshold)")
            else:
                print("   No nodes with connections found in sample")
        
        # Detect communities
        print_section("6. Detect Communities")
        print("Detecting communities using label propagation...")
        communities = service.detect_communities(algorithm='label_propagation')
        print(f"âœ… Detected {len(communities)} communities")
        
        # Show top 10 by size
        community_sizes = sorted([(i, len(c)) for i, c in enumerate(communities)], 
                                key=lambda x: x[1], reverse=True)
        print("\nTop 10 communities by size:")
        for i, (comm_id, size) in enumerate(community_sizes[:10], 1):
            print(f"   {i}. Community {comm_id}: {size} members")
        
        # Analyze largest community
        if len(communities) > 0:
            print_section("7. Analyze Largest Community")
            largest_comm_id = community_sizes[0][0]
            comm_info = service.get_community_info(largest_comm_id)
            
            print(f"Community {largest_comm_id} details:")
            print(f"   â€¢ Size: {comm_info['size']} members")
            print(f"   â€¢ Density: {comm_info['density']:.4f}")
            print(f"   â€¢ Avg Clustering: {comm_info['avg_clustering']:.4f}")
            
            print(f"\n   Top 5 members by influence:")
            for i, member in enumerate(comm_info['members'][:5], 1):
                print(f"      {i}. {member['full_name']} (degree: {member['degree']})")
        
        # Find key connectors
        print_section("8. Find Key Connectors")
        print("Computing betweenness centrality (this may take a moment)...")
        connectors = service.find_key_connectors(min_betweenness=0.001)
        
        if connectors:
            print(f"âœ… Found {len(connectors)} key connectors")
            print("\nTop 5 key connectors:")
            for i, connector in enumerate(connectors[:5], 1):
                print(f"   {i}. {connector['full_name']}")
                print(f"      Betweenness: {connector['betweenness_centrality']:.6f}")
                print(f"      Degree: {connector['degree']}")
                if connector['github_username']:
                    print(f"      GitHub: @{connector['github_username']}")
        else:
            print("   No key connectors found (try lowering threshold)")
        
        # Graph info
        print_section("9. Graph Info & Cache Status")
        info = service.get_graph_info()
        print("Graph status:")
        print(f"   â€¢ Status: {info['status']}")
        print(f"   â€¢ Nodes: {info['num_nodes']}")
        print(f"   â€¢ Edges: {info['num_edges']}")
        print(f"   â€¢ Has embeddings: {info['has_embeddings']}")
        print(f"   â€¢ Has communities: {info['has_communities']}")
        
        cache_status = info['cache_status']
        print(f"\nCache status:")
        print(f"   â€¢ Stats cached: {cache_status['stats_cached']}")
        print(f"   â€¢ Betweenness cached: {cache_status['betweenness_cached']}")
        if cache_status['stats_age_seconds']:
            print(f"   â€¢ Stats age: {cache_status['stats_age_seconds']:.1f} seconds")
        
        # Test incremental update
        print_section("10. Test Incremental Update")
        test_person = {
            'person_id': 'test-uuid-12345',
            'full_name': 'Test Person',
            'headline': 'Software Engineer',
            'location': 'San Francisco',
            'github_username': 'testuser',
            'github_followers': 100,
            'github_repos': 50
        }
        
        print("Adding test person node...")
        added = service.add_person_node(test_person)
        if added:
            print("âœ… Node added successfully")
            print("   (Cache automatically invalidated)")
            
            # Verify
            info_after = service.get_graph_info()
            print(f"   â€¢ New node count: {info_after['num_nodes']}")
        else:
            print("   Node already existed")
        
        # Export graph
        print_section("11. Export Graph")
        import os
        os.makedirs("./exports", exist_ok=True)
        
        print("Exporting graph to JSON...")
        service.export_graph_to_json("./exports/demo_graph.json")
        print("âœ… Exported to ./exports/demo_graph.json")
        
        print("\nExporting graph to GraphML...")
        service.export_graph_to_graphml("./exports/demo_graph.graphml")
        print("âœ… Exported to ./exports/demo_graph.graphml")
        
        # Final stats with betweenness
        print_section("12. Full Statistics (with betweenness - cached)")
        print("Getting full stats (should be fast since betweenness is cached)...")
        full_stats = service.compute_graph_statistics(compute_betweenness=True)
        
        if 'top_central_nodes' in full_stats:
            print("âœ… Full stats with betweenness:")
            print(f"   â€¢ Avg Betweenness: {full_stats['avg_betweenness']:.8f}")
            print(f"\n   Top 3 most central nodes:")
            for i, node in enumerate(full_stats['top_central_nodes'][:3], 1):
                print(f"      {i}. {node['full_name']}")
                print(f"         Betweenness: {node['betweenness']:.6f}")
    
    print_section("DEMO COMPLETE!")
    print("âœ… All graph reasoning features demonstrated successfully!")
    print()
    print("Key takeaways:")
    print("   â€¢ Graph builds in seconds")
    print("   â€¢ Statistics are cached for performance")
    print("   â€¢ Node embeddings enable similarity search")
    print("   â€¢ Community detection works")
    print("   â€¢ Key connectors identified")
    print("   â€¢ Incremental updates supported")
    print("   â€¢ Export to standard formats")
    print()
    print("All features ready for production use via Python!")
    print("=" * 80)

if __name__ == "__main__":
    main()

