# Talent Intelligence Database - Complete Solution

**Last Updated:** October 22, 2025  
**Status:** ‚úÖ Production Ready - High Performance

---

## üéØ What Is This?

A comprehensive talent intelligence database containing:
- **60,045 unique people** with LinkedIn profiles
- **93,387 companies** with full employment history
- **206,697 employment records** (3.4 jobs/person average)
- **8,477 email addresses** across multiple people
- **100,883 GitHub profiles** linked to 333,947 repositories
- **4,210 GitHub‚ÜíPerson links** (4.2% linkage rate)

**Primary Database:** PostgreSQL `talent` @ localhost:5432  
**Performance:** Optimized with 1.3GB of indexes, 60-second query timeouts

---

## üöÄ Quick Start

**For detailed getting started instructions, see [`GETTING_STARTED.md`](GETTING_STARTED.md)**

### Option 1: Query the Database (Recommended)

```bash
# Interactive query menu
./query_database.sh

# Or direct PostgreSQL access
psql -d talent

# Example queries
SELECT COUNT(*) FROM person;
SELECT full_name, linkedin_url FROM person LIMIT 10;
SELECT * FROM person_email LIMIT 10;
SELECT * FROM github_profile ORDER BY followers DESC LIMIT 10;
```

### Option 2: Use the API

```bash
# Start the API server
python run_api.py

# API will be available at:
# http://localhost:8000
# Swagger UI: http://localhost:8000/docs
```

### Option 3: Use the Dashboard

```bash
# Start API server first
python run_api.py

# Open dashboard in browser
open dashboard/index.html
```

---

## üìä Database Structure

### PostgreSQL `talent` Database

#### Core Tables
- **`person`** - 60,045 people with LinkedIn profiles
  - `person_id`, `full_name`, `linkedin_url`, `location`, `headline`, etc.
  - `normalized_linkedin_url` for efficient matching
  - Indexed for fast lookups (< 1ms)

- **`company`** - 93,387 companies
  - `company_id`, `company_name`, `linkedin_url`, `website`, etc.
  - `normalized_linkedin_url` for efficient matching
  - Indexed for fast lookups (< 1ms)

- **`employment`** - 206,697 employment records
  - Full employment history (not just current job)
  - `person_id`, `company_id`, `title`, `start_date`, `end_date`
  - Composite indexes for fast queries (< 10ms)

#### Enhanced Tables (Added Oct 2025)

- **`person_email`** - 8,477 email addresses
  - Multiple emails per person support
  - `person_id`, `email`, `email_type`, `is_primary`
  - 14.1% email coverage

- **`github_profile`** - 100,883 GitHub profiles
  - `person_id`, `github_username`, `github_name`, `followers`, `public_repos`
  - 4,210 linked to people (4.2% linkage)
  - Filtered indexes for fast linked profile queries

- **`github_repository`** - 333,947 repositories
  - `repo_id`, `full_name`, `description`, `language`, `stars`, `forks`
  - Comprehensive repository metadata

- **`github_contribution`** - 7,802 contributions
  - Many-to-many relationship between profiles and repositories
  - `github_profile_id`, `repo_id`, `contribution_count`

#### Graph Tables

- **`edge_coemployment`** - 1.3M+ co-employment relationships
  - Network analysis of people who worked together
  - Optimized with composite indexes (added Oct 22, 2025)
  - Query performance: ~50ms for typical network queries

#### Utility Tables

- **`migration_log`** - Complete audit trail of migration operations
  - Tracks all data consolidation activities

---

## üìÇ Project Structure

```
talent-intelligence-complete/
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ MIGRATION_COMPLETE.md              # Migration results & summary
‚îú‚îÄ‚îÄ config.py                          # Database configuration (PostgreSQL)
‚îÇ
‚îú‚îÄ‚îÄ üü¢ ACTIVE & CURRENT SCRIPTS
‚îÇ   ‚îú‚îÄ‚îÄ migration_scripts/             # ‚úÖ Completed migration scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RUN_MIGRATION.sh          # Master migration script (DONE)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_schema_enhancement.sql # Schema definition
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02_migrate_emails.py      # Email migration (DONE)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 03_migrate_github.py      # GitHub migration (DONE)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 04_deduplicate_people.py  # Deduplication (DONE)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 05_validate_migration.py  # Validation (DONE)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ migration_utils.py        # Shared utilities
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ enrichment_scripts/            # ‚è∏Ô∏è Ready to run (NOT YET EXECUTED)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RUN_ALL_ENRICHMENTS.sh    # Master enrichment script
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_import_sqlite_people.py # Import 15K people from SQLite
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02_enrich_job_titles.py   # Extract titles from headlines
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 03_improve_github_matching_and_emails.py # GitHub matching
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ github_automation/             # ‚è∏Ô∏è Production-ready (NOT YET RUN)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ enrich_github_continuous.py # Main enrichment CLI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ github_client.py          # Rate-limited GitHub API wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ queue_manager.py          # Priority queue management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ enrichment_engine.py      # Core enrichment logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ matcher.py                # Profile matching with confidence scoring
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.py                 # GitHub automation config
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ api/                           # ‚úÖ Built & Functional
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                   # FastAPI application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routers/                  # people, companies, graph, query, stats endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crud/                     # Database operations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models/                   # Pydantic models
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ dashboard/                     # ‚úÖ Built & Functional
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.html                # Search interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.js                    # Frontend logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ style.css                 # Styling
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ run_api.py                    # ‚úÖ API server launcher
‚îÇ   ‚îú‚îÄ‚îÄ query_database.sh             # ‚úÖ Interactive query menu
‚îÇ   ‚îú‚îÄ‚îÄ query_database_secure.py     # ‚úÖ Secure query interface
‚îÇ   ‚îú‚îÄ‚îÄ comprehensive_analysis.sql    # ‚úÖ Database analysis queries
‚îÇ   ‚îú‚îÄ‚îÄ generate_audit_report.py      # ‚úÖ Database audit generator
‚îÇ   ‚îú‚îÄ‚îÄ generate_quality_metrics.py   # ‚úÖ Quality metrics
‚îÇ   ‚îú‚îÄ‚îÄ check_data_quality.py         # ‚úÖ Data quality checks
‚îÇ   ‚îú‚îÄ‚îÄ backup_database.py            # ‚úÖ Database backup utility
‚îÇ   ‚îú‚îÄ‚îÄ populate_coemployment_graph.py # ‚úÖ Graph population
‚îÇ   ‚îú‚îÄ‚îÄ prep_company_discovery.py     # ‚úÖ Company discovery prep
‚îÇ   ‚îî‚îÄ‚îÄ analyze_database_overlap.py   # ‚úÖ Overlap analysis
‚îÇ
‚îú‚îÄ‚îÄ üü° DIAGNOSTIC TOOLS (Debugging)
‚îÇ   ‚îú‚îÄ‚îÄ diagnostic_tools/             # Debugging and diagnostic scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ diagnose_github.py        # GitHub debugging
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ investigate_talent_schema.py # Schema investigation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ diagnose_duplicates.sh    # Duplicate diagnostics
‚îÇ   ‚îÇ
‚îú‚îÄ‚îÄ üî¥ ARCHIVED & LEGACY
‚îÇ   ‚îú‚îÄ‚îÄ archived_implementations/     # Historical scripts (SQLite-era)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ build_candidate_database.py # Built SQLite people table
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ build_company_database.py  # Built SQLite company table
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fix_employment_duplicates.py # Employment deduplication fix
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fix_github_schema.py      # GitHub schema fix
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ day1_setup.sh             # Phase 1 setup (completed)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ day2_setup.sh             # Phase 2 setup (completed)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RUN_ME.sh                 # Original SQLite builder
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RUN_PHASE2.sh             # Company phase (legacy)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RUN_PHASE3.sh             # GitHub phase (legacy)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                 # Archive documentation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ legacy_scripts/               # Overlapping functionality
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ github_enrichment.py      # Original enrichment script
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ github_api_enrichment.py  # API-based enrichment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ build_github_enrichment.py # Build enrichment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ github_queue_manager.py   # Old queue manager
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ match_github_profiles.py  # Standalone matching script
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ import_github_orgs.py     # Standalone GitHub org import
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                 # Legacy documentation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ archived_databases/           # Archived legacy databases
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sqlite/                   # SQLite databases
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgresql_dumps/         # PostgreSQL backups
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ backups/                      # Database backups
‚îÇ       ‚îî‚îÄ‚îÄ *.db.gz                   # Compressed backups
‚îÇ
‚îî‚îÄ‚îÄ [Documentation, logs, and configuration files]
```

---

## üîß Configuration

### Environment Variables

Set these in `.env` file or environment:

```bash
# PostgreSQL Connection (primary database)
PGHOST=localhost
PGPORT=5432
PGDATABASE=talent
PGUSER=charlie.kerr
PGPASSWORD=  # Optional for local connections

# GitHub API (for enrichment)
GITHUB_TOKEN=your_token_here
```

### Check Configuration

```bash
python3 config.py
```

This will show:
- Database connection status
- Database contents summary
- Configuration validation
- Archived database locations

---

## üéì Key Scripts & Commands

### Database Queries

```bash
# Interactive query menu
./query_database.sh

# Direct PostgreSQL access
psql -d talent

# Count records
psql -d talent -c "SELECT COUNT(*) FROM person;"
psql -d talent -c "SELECT COUNT(*) FROM company;"
psql -d talent -c "SELECT COUNT(*) FROM person_email;"
psql -d talent -c "SELECT COUNT(*) FROM github_profile;"
```

### Example SQL Queries

```sql
-- People with emails
SELECT p.full_name, pe.email 
FROM person p
JOIN person_email pe ON p.person_id = pe.person_id
LIMIT 10;

-- People with GitHub profiles
SELECT p.full_name, gp.github_username, gp.followers
FROM person p
JOIN github_profile gp ON p.person_id = gp.person_id
ORDER BY gp.followers DESC
LIMIT 10;

-- Employment history for a person
SELECT p.full_name, c.company_name, e.title, e.start_date, e.end_date
FROM person p
JOIN employment e ON p.person_id = e.person_id
JOIN company c ON e.company_id = c.company_id
WHERE p.full_name ILIKE '%John Doe%'
ORDER BY e.start_date DESC;

-- GitHub contributions
SELECT p.full_name, gp.github_username, gr.full_name as repo, gc.contribution_count
FROM github_contribution gc
JOIN github_profile gp ON gc.github_profile_id = gp.github_profile_id
JOIN github_repository gr ON gc.repo_id = gr.repo_id
JOIN person p ON gp.person_id = p.person_id
ORDER BY gc.contribution_count DESC
LIMIT 10;
```

### GitHub Enrichment

```bash
# Use the production-ready GitHub automation (recommended)
cd github_automation
python3 enrich_github_continuous.py

# Or use the enrichment scripts (alternative approach)
cd enrichment_scripts
./RUN_ALL_ENRICHMENTS.sh
```

**Note**: Legacy GitHub enrichment scripts have been moved to `legacy_scripts/` directory. Use the `github_automation/` package for new work.

### Database Backups

```bash
# Manual backup
pg_dump -h localhost -d talent | gzip > backups/talent_$(date +%Y%m%d).sql.gz

# Automated backup script
python3 backup_database.py
```

---

## üìö Documentation

### Primary References
- **`README.md`** - This file (primary documentation)
- **`GETTING_STARTED.md`** - Consolidated getting started guide
- **`TESTING.md`** - Testing documentation
- **`MIGRATION_COMPLETE.md`** - Migration results & summary

### API & Dashboard
- **`api/README.md`** - API documentation
- **`dashboard/README.md`** - Dashboard documentation
- **`API_AND_DASHBOARD_COMPLETE.md`** - Implementation status

### GitHub Automation
- **`github_automation/README.md`** - GitHub automation package
- **`GITHUB_AUTOMATION_COMPLETE.md`** - Complete implementation guide
- **`IMPLEMENTATION_STATUS.md`** - Current status

### Audit Results
- **`audit_results/EXECUTIVE_FINDINGS.md`** - Database audit findings
- **`audit_results/AUDIT_COMPLETE_SUMMARY.md`** - Audit summary
- **`AUDIT_RESULTS_README.md`** - Audit results index

### Archived Documentation
- **`archived_documentation/`** - Historical and overlapping docs
  - `historical/` - Pre-migration documentation (SQLite-era)
  - `overlapping/` - Consolidated documentation (redundant content)
  - `person_specific/` - Person-specific summaries (historical)

### Script Organization
- **`SCRIPT_ORGANIZATION_COMPLETE.md`** - Script organization summary
- **`archived_implementations/README.md`** - Archived scripts documentation
- **`legacy_scripts/README.md`** - Legacy scripts documentation
- **`diagnostic_tools/README.md`** - Diagnostic tools documentation

---

## üîÑ Migration History

### October 20, 2025: Database Consolidation

**Problem:** Data fragmentation across 12 databases (3 SQLite + 9 PostgreSQL)

**Solution:** Consolidated all data into ONE PostgreSQL `talent` database

**Results:**
- ‚úÖ Schema enhanced with `person_email`, `github_profile`, `github_repository`, `github_contribution` tables
- ‚úÖ 1,014 emails migrated from SQLite
- ‚úÖ 17,534 GitHub profiles migrated
- ‚úÖ 374 repositories and 7,802 contributions migrated
- ‚úÖ 0 duplicates found (database was already clean)
- ‚úÖ 8 PostgreSQL databases archived (86M total)
- ‚úÖ SQLite database archived to `archived_databases/`
- ‚úÖ Configuration updated to use PostgreSQL

**Primary Database:** PostgreSQL `talent` @ localhost:5432

See `MIGRATION_COMPLETE.md` for full details.

---

## üóÑÔ∏è Archived Databases

All legacy databases have been archived to `archived_databases/`:

### Archived SQLite
- `talent_intelligence.db` ‚Üí `archived_databases/sqlite/`

### Archived PostgreSQL (Dumps)
- `talent_intelligence` (9.8M)
- `talent_intel` (852K)
- `talent_graph` (4.0K)
- `talentgraph` (4.0K)
- `talentgraph2` (640K)
- `talentgraph_development` (4.0K)
- `tech_recruiting_db` (26M)
- `crypto_dev_network` (49M)

All dumps are in `archived_databases/postgresql_dumps/`

### To Restore an Archived Database

```bash
# Restore from compressed dump
gunzip -c archived_databases/postgresql_dumps/talent_intelligence_20251020_161335.sql.gz | psql -d new_database_name
```

### To Drop Archived Databases (Optional)

```bash
# Review and optionally drop archived PostgreSQL databases
cd archived_databases
./drop_archived_databases.sh
```

---

## üìà Data Quality Metrics

| Metric | Value | Status |
|--------|-------|--------|
| **People** | 60,045 | ‚úÖ Excellent |
| **Companies** | 93,387 | ‚úÖ Excellent |
| **Employment Records** | 206,697 | ‚úÖ (3.4 jobs/person avg) |
| **LinkedIn Coverage** | 100% | ‚úÖ All people have LinkedIn |
| **Email Coverage** | 14.1% (8,477) | ‚ö†Ô∏è Moderate |
| **GitHub Profiles** | 100,883 | ‚úÖ Excellent scale |
| **GitHub Linked** | 4.2% (4,210) | ‚ö†Ô∏è Growing (up from 3.6%) |
| **Duplicates** | 0 | ‚úÖ Clean |
| **Data Integrity** | 100% | ‚úÖ Perfect |
| **Query Performance** | < 60s | ‚úÖ Optimized (Oct 22) |

---

## üöß Future Enhancements

### Potential Next Steps

1. **Import More People**
   - Migrate the 15,350 people from SQLite into PostgreSQL
   - This would bring email coverage to ~45%

2. **Enhanced GitHub Enrichment**
   - API-based profile enrichment
   - Repository activity tracking
   - Contribution analytics

3. **Company Enrichment**
   - Website scraping
   - Funding data integration
   - LinkedIn company data

4. **Advanced Analytics**
   - Career path analysis
   - Skills mapping from job titles
   - Network analysis (coemployment graphs)

5. **Data Quality Improvements**
   - Email validation and enrichment
   - LinkedIn profile refreshing
   - Deduplication across different name variations

---

## üõ†Ô∏è Development

### Adding New Features

1. **Update Schema:**
   ```sql
   -- Add new tables or columns
   psql -d talent -f your_schema_changes.sql
   ```

2. **Update Config:**
   - Edit `config.py` to add new settings
   - Test with `python3 config.py`

3. **Create Migration Script:**
   - Follow patterns in `migration_scripts/`
   - Use `migration_utils.py` for common functions
   - Log all operations to `migration_log` table

### Best Practices

- Always backup before schema changes: `pg_dump -d talent > backup.sql`
- Use transactions for data modifications
- Log all operations to `migration_log` table
- Update documentation after significant changes
- Test queries on small datasets first

---

## üìû Support & Troubleshooting

### Common Issues

**Issue:** Can't connect to PostgreSQL database

```bash
# Check if PostgreSQL is running
pg_isready

# Check connection
psql -d talent

# Check config
python3 config.py
```

**Issue:** Need to access archived SQLite data

```python
from config import get_sqlite_connection
conn = get_sqlite_connection()
cursor = conn.cursor()
cursor.execute("SELECT COUNT(*) FROM people")
print(cursor.fetchone())
```

**Issue:** Need to restore an archived database

```bash
# List available backups
ls -lh archived_databases/postgresql_dumps/

# Restore to a new database
gunzip -c archived_databases/postgresql_dumps/talent_intel_20251020_161335.sql.gz | psql -d restored_talent_intel
```

### Migration Logs

View detailed migration history:

```sql
-- In PostgreSQL
SELECT * FROM migration_log ORDER BY started_at DESC;
```

### Getting Help

1. Review `MIGRATION_COMPLETE.md` for migration details
2. Check `audit_results/EXECUTIVE_FINDINGS.md` for database analysis
3. Review migration logs in `migration_log` table
4. Check `migration_scripts/README.md` for script documentation

---

## ‚úÖ System Status

**Current State (October 22, 2025):**
- ‚úÖ Single primary database (PostgreSQL `talent`)
- ‚úÖ 60K+ people, 100K+ GitHub profiles
- ‚úÖ Performance optimized (emergency fix Oct 22)
- ‚úÖ 1.3GB of optimized indexes
- ‚úÖ Query timeouts implemented (60s API, 5min pool)
- ‚úÖ Connection pooling (5-50 connections)
- ‚úÖ No duplicates
- ‚úÖ 100% data integrity
- ‚úÖ Dashboard fully functional
- ‚úÖ API fully operational

**Primary Database:** `postgresql://charlie.kerr@localhost:5432/talent`

**Recent Updates (Oct 22, 2025):**
- üöÄ Major performance optimization completed
- üìä 51 hung queries terminated
- üîß VACUUM ANALYZE on all critical tables
- üìà 1.3GB of new indexes added
- ‚è±Ô∏è Query timeouts implemented
- üîó GitHub profile matching improved (+641 links)

**System Ready:** ‚úÖ Production Ready - High Performance

---

## üìÑ Quick Reference

For current database state, see: `QUICK_STATS.txt`  
For performance details, see: `reports/current/PERFORMANCE_FIX_SUMMARY.md`  
For complete audit, see: `REPOSITORY_AUDIT_2025.md`

---

## üìÑ License

Internal use only - Talent Intelligence Database

---

**Last Updated:** October 22, 2025  
**Database Version:** PostgreSQL `talent` (Post-performance-optimization)  
**Performance Status:** ‚úÖ Optimized & Production Ready
